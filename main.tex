\documentclass[a4paper, 11pt, french]{article}
\usepackage[utf8]{inputenc}
\usepackage[margin=1.5cm,headheight=52pt,includeheadfoot]{geometry} 
\usepackage{mathtools} %amsmath mis à jour
\usepackage{amssymb} %symboles mathématiques
\usepackage{latexsym}
\usepackage{stmaryrd} 
\usepackage{babel} %gestion du français
\usepackage{amsthm}

\renewcommand{\labelitemi}{∙}

\newcommand{\abs}[1]{\vert#1\vert}
\newcommand{\norme}[1]{\Vert#1\Vert}
\newcommand{\scal}[2]{<#1\vert#2>}

\theoremstyle{definition}
\newtheorem{definition}{Définition}

\newtheorem{theorem}{Théorème}

\newtheorem{property}{Propriété}

\renewcommand\qedsymbol{$\blacksquare$}


\title{TER M1 : \\ NN and RKHS}
\author{Matthieu Denis}

\begin{document}
	
	\maketitle
	\newpage
	
	\tableofcontents
	\newpage
	
	\section{Reproducing Kernel Hilbert space (RKHS)}
	
	Nous allons dans cette section s'intéresser aux RKHS, des espaces de Hilbert réels qui satisfont certaines propriétés, et qui ont des applications intéressantes en machine learning. Par exemple, on montrera un théorème qui nous permet de simplifier un problème de minimisation de risque empirique de dimension infini à un problème en dimension fini. Ou encore des applications dans plusieurs algorithmes de ML, les transformant d'algorithmes linéaires à non-linéaires à très faible prix, et d'autres encore. \\
	
	Soit $X$ un ensemble quelconque, $H$ un espace de Hilbert de fonctions réelles sur $X$, muni de l'addition point par point ainsi que de la multiplication par scalaire point par point. On introduit aussi une forme linéaire qui à chaque fonction de $H$ l'évalue en un certain point $x \in X$,
	
	\[L_x : f \mapsto f(x) \; \forall f \in H\]
		
	\begin{definition}[RKHS]
		On dit d'un espace de Hilbert qu'il est un RKHS si $\forall x \in X$, $L_x$ est continue sur $H$, ou encore si $L_x$ est bornée sur $H$, i.e
		\[\forall x \in X, \;  \exists M_x > 0, \; \forall f \in H \text{ t.q } |L_x(f)| \coloneqq |f(x)| \leq M_x ||f||_H\]
	\end{definition}

	Dans ce qui suit, $H$ sera un RKHS.

	\begin{property}[Convergence en norme dans un RKHS implique pointwise convergence]
		Soient $f_n, \, f \in H$. Si $f_n \stackrel{H}{\to} f$, alors $\forall x \in X, \; f_n(x) \to f(x)$.
	\end{property}
	\begin{proof}
		\begin{align*}
			\forall x \in X, \; |f_n(x) - f(x)| = |L_x(f_n) - L_x(f)| = |L_x(f_n - f)| \leq M_x||f_n - f||_H
		\end{align*}
	\end{proof}

	\begin{definition}[Noyau / Kernel]
		Une fonction $k : X \times X \to \mathbb{R}$ est un noyau si
		\[ \exists \phi : X \to H \text{ t.q } k(x,y) = \langle \phi (x), \phi(y) \rangle_H \; \forall x,y \in X \]
	\end{definition}

	\begin{property}
		Tout noyau $k$ est symétrique défini positif.
	\end{property}
	\begin{proof}
		\begin{itemize}
			\item[$\bullet$] Symétrie : découle de la symétrie du produit scalaire.			
			\item[$\bullet$] Défini positif : \\
			$\forall x_1, \cdots, x_n \in X, \forall c_1, \cdots, c_n \in X$
			\[\sum_{i,j=1}^{n} c_i c_j k(x_i, x_j) = \left\langle \sum_{i=1}^{n} c_i \phi(x_i), \sum_{j=1}^{n} c_j \phi(x_j) \right\rangle_H = \left|\left|\sum_{i=1}^{n} c_i \phi(x_i)\right|\right|_H^2\ \geq 0\]
		\end{itemize}
	\end{proof}

	\begin{definition}[Noyau reproduisant / Reproducing kernel]
		Une fonction $k : X \times X \to \mathbb{R}$ est un noyau reproduisant de H si $\forall x \in X, \, f \in H$ :
		\begin{itemize}
			\item[$\bullet$] $k(\cdot, x) \in H$			
			\item[$\bullet$] $f(x) = \langle f, k(\cdot, x) \rangle_H$
		\end{itemize}
	\end{definition}

	\newpage

	\begin{property}
		Tout noyau reproduisant $k$ est un noyau.
	\end{property}
	\begin{proof}
		En prenant $f = k(\cdot, y) \in H$ pour un certain $y \in X$ dans la définition, on a en particulier $\forall x, y \in X \; k(x,y) = \langle k(\cdot, x), k(\cdot, y) \rangle_H$. Ici $\phi(u) = k(\cdot, u) \; \forall u \in X$.
	\end{proof}

	\begin{theorem}[Théorème de représentation de Riesz]
		Soient :
		\begin{itemize}
			\item[$\bullet$] $H$ un espace de Hilbert réel, muni de son produit scalaire $\langle \cdot, \cdot \rangle_H$
			\item[$\bullet$] $L \in H'$ une forme linéaire continue sur $H$.
		\end{itemize}
		Alors \[\exists ! \, g \in H, \; \forall f \in H, \; L(f) = \langle f, g \rangle_H\]
	\end{theorem}

	\begin{property}[Existence et Unicité]
		Il existe un unique noyau reproduisant de $H$.
	\end{property}
	\begin{proof}
		Montrons l'existence puis l'unicité : \\
		\begin{itemize}
			\item[$\bullet$]
			On applique le théorème de Riesz à $L_x$ :
			\[\forall x \in X, \; \exists ! \, k_x \in H, \; \forall f \in H, \; f(x) = L_x (f) = \langle f, k_x \rangle_H\]
			Soit $k(y, x) \coloneqq k_x(y) \; \forall x,y \in X$, alors $\forall x \in X, \, f \in H, \; k(\cdot, x) \in H$ et  $f(x) = \langle f, k(\cdot, x) \rangle_H$.
			
			Donc $k$ est un noyau reproduisant de $H$.
			
			\item[$\bullet$]
			Soient $k_1$ et $k_2$ deux noyau reproduisant de $H$. Alors $\forall x \in X, \, f \in H$
			\[\langle f, k_1(\cdot, x) - k_2(\cdot, x) \rangle_H = f(x) - f(x) = 0\]
			En prenant $f = k_1(\cdot, x) - k_2(\cdot, x)$, on a :
			\[\left|\left| k_1(\cdot, x) - k_2(\cdot, x) \right|\right|_H^2 = 0\]
			C'est-à-dire que $k_1 = k_2$ : le noyau reproduisant de $H$ est unique.
		\end{itemize}
	\end{proof}

	\begin{theorem}[Lien des deux visions]
		$H$ est un RKHS si et seulement si il existe un unique noyau reproduisant de $H$.
	\end{theorem}
	\begin{proof}
		\begin{itemize}
			\item[$\bullet$] $(\Rightarrow)$
				Donné par la propriété 2.
			\item[$\bullet$] $(\Leftarrow)$ $\forall x \in X, \, f \in H$
				\[|L_x(f)| = |f(x)| = |\langle f, k(\cdot, x) \rangle_H| \leq ||f||_H ||k(\cdot,x)||_H = \underbrace{\sqrt{k(x,x)}}_{\text{$M_x$}} ||f||_H\]
				$M_x$ ne dépendant pas de $f$, on a bien l'inégalité $\forall x \in X, \;  \exists M_x > 0, \; \forall f \in H$, i.e $H$ est un RKHS.
		\end{itemize}
	\end{proof}
	
	\begin{theorem}[Théorème de Moore-Aronszajn]
	 	Soit K un kernel. Alors il existe un unique espace de Hilbert $H$ de fonctions sur $X$ pour lequel $K$ est un noyau reproduisant.
	\end{theorem}
	\begin{proof}
		$\forall x \in X$, on pose $K_x(\cdot) \coloneqq K(x, \cdot)$. Soit $H_0$ le sous-espace vectoriel engendré par \\ $\{K_x : x \in X\}$. Sous couvert d'existence de $H$, la reproducing property de $K$ nous assure que $\forall x, y \in X, \; K(x, y) = \langle K_x, K_y \rangle_H$. On a de même :
		\[\left\langle \sum_{i=1}^{n} a_i K_{x_i}, \sum_{j=1}^{m} b_j K_{x_j} \right\rangle_H = \sum_{i=1}^{n} \sum_{j=1}^{m} a_i b_j K(x_i, x_j)\]
		
		Il est alors naturel de définir pour toutes fonctions de $H_0$ : $f \coloneqq \sum_{i=1}^n a_i K_{x_i}$ et $g \coloneqq \sum_{j=1}^m b_i K_{y_j}$ 
		\[\langle f, g \rangle_{H_0} \coloneqq \sum_{i=1}^{n} \sum_{j=1}^{m} a_i b_j K(x_i, x_j) = \sum_{i=1}^n a_i g(x_i) = \sum_{j=1}^m b_j f(y_j)\]
		
		Déjà, $K_x \in H_0$, et comme $f(x) = \langle f, K_x \rangle_{H_0} < \infty$, $\langle \cdot, \cdot \rangle_{H_0}$ est bien défini, billinéaire, est symétrique et positif, et on a la reproducing property :
		
		\[\forall x \in X, \; f \in H_0, \; \langle f, K_x \rangle_{H_0} = \sum_{i=1}^n a_i K_x(x_i) = \sum_{i=1}^n a_i K_{x_i}(x) = f(x)\]
		
		Il reste à montrer que $||f||_{H_0} = 0 \Rightarrow f = 0$ pour que ca soit un produit scalaire :
		
		\[|f(x)| = |\langle f, K_x \rangle_{H_0}| \leq ||f||_{H_0} ||K_x||_{H_0} = ||f||_{H_0} \sqrt{K(x,x)} \]
		
		Donc $\langle \cdot, \cdot \rangle_{H_0}$ est un produit scalaire. \\
		
		On complète $H_0$ avec le produit scalaire $\langle \cdot, \cdot \rangle_{H_0}$ en $H$, et toutes les propriétés sont gardées. \\
		
		Unicité : Supposons $G$ un autre espace de Hilbert pour lequel $K$ est un noyau reproduisant. On a alors  
		\[\forall x, y \in X, \; \langle K_x, K_y \rangle_H = K(x, y) = \langle K_x, K_y \rangle_G\]
		
		Par linéarité, $\langle \cdot, \cdot \rangle_H = \langle \cdot, \cdot \rangle_G$ sur tout $H_0$. {\bf FINIR PREUVE UNICITE}
	\end{proof}
	
	Tout compte fait, on a montré que :
	\[H \text{ est un RKHS} \Leftrightarrow \exists ! \; k \text{ noyau reproduisant de $H$} \Leftrightarrow k \text{ est un noyau}\]
	
	
	
	
	
	
	
	
	
	
	
	
	
	\section{Introduction : Réseau de neurone simple}
	
	Commencons par étudier un NN très simple : une fonction
	$\Phi : (\mathbb{R}^m \times \mathbb{R}^{m \times m} \times \mathbb{R}^m) \times \mathbb{R} \to \mathbb{R}$ combinaison d'applications linéaires, sans non linéarités  intermédiaires  :
	
	\[ \Phi ((\beta, A, u), x) \coloneqq \frac{1}{m^{\alpha}} \beta^T
		 \left( \frac{1}{m^{\gamma}} A \right) u x \]
		 
	On initialise $\theta^0 \coloneqq (\beta^0, A^0, u^0)$ de manière standarde : 
	$ \forall i,j \in \{1, \cdots, m\} , \; u_i^0, A_{ij}^0, \beta_i^0 \sim_{iid} N(0, 1)$
	
	Nous montrerons quelques propriétés asymptotiques en la largeur des couches, et sur l'évolution des paramètres lors du premier pas de la descente de gradient.
	
	\subsection{Lois suivant la largeur des couches $m$}
	
	\begin{itemize}
		
		\item[$\bullet$] Loi de $ || u^0 ||_2^2$ pour $m$ grand \\
		
		Comme $ || u^0 ||_2^2 = \sum_{i=1}^{m} (u_i^0)^2 \sim \chi^2 (m)$ et $ u_i^0 \sim \chi^2 (1)$, en appliquant le TCL aux $ u_i^0 $, on a :
		
		\[
			\frac{|| u^0 ||_2^2 - m}{\sqrt{2m}} \sim_{m \to \infty}  N(0, 1)
		\]
	
		C'est-à-dire que $ || u^0 ||_2^2 \sim N(m, 2m) $  pour $m$ grand. \\
		
		\item[$\bullet$] Loi de $ \left(\frac{1}{m^{\gamma}} A^0 \right) u^0 x $ sachant $ u^0 $ \\
		
		$ (A^0 u^0)_i = \sum_{j=1}^m A_{ij}^0 u_j^0$. En sachant $u^0$, comme 
		$A_{i \cdot}^0$ est un vecteur gaussien, $(A^0 u^0)_i \sim  N(0, || u^0 ||_2^2 ) $. 
	
		De même, part indépendance des $A_{ij}^0$, les $(A^0 u^0)_i$ sont indépendants et $A^0 u^0 \sim N(0_m, || u^0 ||_2^2 \; Id_m)$.
		
		Ainsi, $ \left(\frac{1}{m^{\gamma}} A^0 \right) u^0 x $ | $ u^0 
		\sim  N(0_m, \left( \frac{x}{m^{\gamma}} \right)^2 || u^0 ||_2^2 \; Id_m) $. \\
		
		\item[$\bullet$] Loi de $ \left(\frac{1}{m^{\gamma}} A^0 \right) u^0 x $ \\
		
		HISTOIRE DE MEME TRIBU ENGENDREE PAR U0 ET LE RESTE IMPLIQUE QUE CEST LA MEME CHOSE DECONDITIONNEE
		
		Donc $ \left(\frac{1}{m^{\gamma}} A^0 \right) u^0 x  \sim  N(0_m, 
		\left( \frac{x}{m^{\gamma}} \right)^2 || u^0 ||_2^2 \; Id_m) $. \\
		
		\item[$\bullet$] Loi de $ || \left(\frac{1}{m^{\gamma}} A^0 \right) u^0 x \; ||_2^2 $ pour $m$ grand
		 \\
		
		On a $ \left( \left(\frac{1}{m^{\gamma}} A^0 \right) u^0 x \right)_i^2 \sim  
		\left( \frac{x}{m^{\gamma}} \right)^2 || u^0 ||_2^2 \cdot  \chi^2 (1) $, d'espérance 
		$ \mu \coloneqq \left( \frac{x}{m^{\gamma}} \right)^2 || u^0 ||_2^2  $  et de variance \\
		$ \sigma^2 \coloneqq 2 \left( \left( \frac{x}{m^{\gamma}} \right)^2 || u^0 ||_2^2  \right)^2 $.
		
		Donc en appliquant le TCL à ceux ci, on a :
		
		 \[
		 	\frac{|| \left(\frac{1}{m^{\gamma}} A^0 \right) u^0 x \; ||_2^2 - m \mu}{\sigma \sqrt{m}} \sim_{m \to \infty}  N(0, 1)
		 \]
		
		C'est-à-dire que $|| \left(\frac{1}{m^{\gamma}} A^0 \right) u^0 x \; ||_2^2 \sim 
		N(m \mu, m \sigma^2) $  pour $m$ grand. \\
		
		\newpage
		
		\item[$\bullet$][$\bullet$] Loi de $ \frac{1}{m^{\alpha}} (\beta^0)^T x_2 $ sachant $x_2$, avec 
		$x_2 = \left(\frac{1}{m^{\gamma}} A^0 \right) u^0 x$ \\
		
		On a 
		$ \frac{1}{m^{\alpha}} (\beta^0)^T x_2 | x_2 \sim N(0,  \frac{1}{m^{2\alpha}}||x_2||_2^2) $
		\\
		
		\item[$\bullet$][$\bullet$] Loi de $ \frac{1}{m^{\alpha}} (\beta^0)^T x_2 $ \\
		
		On a 
		$ \frac{1}{m^{\alpha}} (\beta^0)^T x_2 \sim N(0,  \frac{1}{m^{2\alpha}}||x_2||_2^2) $
		\\
		
	\end{itemize}

	\subsection{Choix de $\gamma$}
	
	On regarde la variance de chaque composante de 
	$ \left(\frac{1}{m^{\gamma}} A^0 \right) u^0 x $ pour $m$ grand : 
	$ Var = x^2 m^{-2\gamma} m $ comme $|| u^0 ||_2^2$ se comporte en $m$ pour $m$ grand. Or on ne veut pas qu'elle tende vers 0 ou l' infini lorsque $m$ tend vers l'infini car $\Phi$ prendrait des valeurs de 0 ou l'infini, ce qui impose le choix $\gamma = 1/2$ \\
	
	Dans la suite, on prendra $\gamma = 1/2$.

	\subsection{Gradients}
	
	Trivialement,
	
	\[\nabla_{\beta} \Phi = \frac{x}{m^{\alpha + 1/2}} A u\]
	
	\[\nabla_u \Phi = \frac{x}{m^{\alpha + 1/2}} A^T \beta\]

	\[\nabla_A \Phi = \frac{x}{m^{\alpha + 1/2}} \beta u^T\]
	
	Etudions les ordres de grandeur des normes correspondantes à l'initialisation pour $m$ grand :
	
	On va simplement utiliser les approximations données par la loi des grands nombres :
	 $||u^0|| \simeq \sqrt{m}$, et comme vu plus haut, 
	 $||A^0 u^0|| \simeq \sqrt{m}||u^0|| \simeq m$. Ainsi $||\nabla_{\beta} \Phi (\theta^0, x)|| \sim m^{-\alpha - 1/2} \cdot m = m^{1/2 - \alpha}$.
	 
	 \[||\nabla_{\beta} \Phi (\theta^0, x)|| \sim m^{1/2 - \alpha}\]
	 
	 Exactement de la même manière, on aboutit à :
	 
	 \[||\nabla_u \Phi (\theta^0, x)|| \sim m^{1/2 - \alpha}\]
	 
	 Pour $A$, on prend la norme de Frobenius : la LGN nous dit que $||\beta^0 (u^0)^T|| \simeq m$ et donc :
	 
	 \[||\nabla_A \Phi (\theta^0, x)||_F \sim m^{1/2 - \alpha}\]
	 
	 \subsection{Descente de gradient}
	 
	 On va étudier ici le premier pas de descente de gradient.
	 
	 Posons une fonction de perte $F : \mathbb{R} \rightarrow \mathbb{R}$ t.q $F'(0) \neq 0$ et 
	 $\Delta F \coloneqq F(\Phi(\theta^1, x)) - F(\Phi(\theta^0, x))$, avec 
	 $\theta^1 \coloneqq \theta^0 - \eta \nabla_{\theta} F(\Phi(\theta^0, x))$
	 
	 Il semble honnête de prendre $\eta$ dépendant de $m$, le produit scalaire final ayant plus de chance d'exploser en grande dimension. Prenons $\eta \coloneqq m^a$, $a \in \mathbb{R}$ \\
	 
	 \subsubsection{Choix de $\eta$}
	 
	 On veut que $\Delta F$ ne diverge pas ni ne tende vers 0 lorsque m tend vers l'infini.
	 
	 Pour cela, on utilise l'approximation 
	 $\Delta F \simeq \; < \Delta \theta, \nabla_{\theta} F(\Phi(\theta^0, x)) >$.
	 
	 On a 
	 \[
	 \Delta F \simeq \; < -\eta \nabla_{\theta} F(\Phi(\theta^0, x)) , \nabla_{\theta} F(\Phi(\theta^0, x)) > = -\eta || \nabla_{\theta} F(\Phi(\theta^0, x)) ||^2
	 \]
	 
	 \[
	 \nabla_{\theta} F(\Phi(\theta^0, x)) = 
	 \underbrace{F'(\Phi(\theta^0, x))}_\text{constante en $m$} 
	 \cdot \nabla_{\theta} \Phi(\theta^0, x)
	 \]
	 
	 Or $ || \nabla_{\theta} \Phi(\theta^0, x) ||^2 = || \nabla_{\beta} \Phi(\theta^0, x) ||^2 + || \nabla_{u} \Phi(\theta^0, x) ||^2 + || \nabla_{A} \Phi(\theta^0, x) ||^2$ \\
	 
	 Donc pour $m$ grand :
	 \begin{align*}
	 	\Delta F &\simeq -\eta || \nabla_{\theta} F(\Phi(\theta^0, x)) ||^2 \\
	 	&\sim \eta || \nabla_{\theta} \Phi(\theta^0, x) ||^2 \\
	 	&\simeq \eta (3 \cdot (m^{1/2 - \alpha})^2) \\
	 	&\simeq m^a \cdot m^{1 - 2\alpha}
	 \end{align*}	
	
	Ce qui impose le choix $a = 2\alpha - 1$ \\
	
	\subsubsection{Ordres de grandeur des écarts relatifs}

	Pour cela introduisons $\Delta \theta \coloneqq \theta^1 - \theta^0$. Remarquons que $	\Delta \theta = - \eta \nabla_{\theta} F(\Phi(\theta^0, x))$.
	
	\begin{align}
		||\Delta u|| &\sim \eta || \nabla_u \Phi(\theta^0, x) || \\
		&\sim m^{2\alpha - 1} \cdot m^{1/2 - \alpha} \\
		&\sim m^{\alpha - 1/2}
	\end{align}

	Ce qui nous donne un ordre de grandeur de l'écart relatif :
	
	\[\frac{||\Delta u||}{||u^0||} \sim m^{\alpha - 1}\]
	
	On a le même résultat pour l'écart relatif de $\beta^0$ :
	
	\[\frac{||\Delta \beta||}{||\beta^0||} \sim m^{\alpha - 1}\]
	
	Pour $A$, la LGN nous donne $||A^0||_F \simeq m$ pour $m$ grand, on a alors par les mêmes calculs :
	
	\[\frac{||\Delta A||_F}{||A^0||_F} \sim m^{\alpha - 3/2}\]
	
	Concernant l'écart entrywise de A, on a $|\Delta A_{ij}| \sim \eta | (\nabla_A \Phi(\theta^0, x))_{ij} | \sim m^{2\alpha - 1} \cdot m^{-1/2 - \alpha} \cdot 1 \sim m^{\alpha - 3/2}$ car $|\beta^0 (u^0)^T| \sim 1$. $|A_{ij}| \sim 1$, donc :

	\[\frac{|\Delta A_{ij}|}{|A_{ij}|} \sim m^{\alpha - 3/2}\]
	
	Maintenant avec la norme opétateur : le corollaire 7.9 du cours de MIA2 nous donne une majoration sur $|A^0|_{op}$ : $|A^0|_{op} \leq \sqrt{m} + 7\sqrt{m + \xi} \sim \sqrt{m}$, avec $\xi \sim Exp(1)$.
	
	De plus, on peut trouver la la norme opérateur de $\Delta A$ comme suit : tout le travail est de trouver $|\beta^0 (u^0)^T|_{op} \coloneqq \sup \{||\beta^0 (u^0)^T x|| \; 
	\text{avec }||x|| = 1\}$.
	
	\begin{align}
		(\beta^0 (u^0)^T x)_{ij} &= \beta^0_i u^0_j \\
		(\beta^0 (u^0)^T x)_i &= \sum_{j=1}^{m} (\beta^0 (u^0)^T x)_{ij} \cdot x_j \\
		&= \beta^0_i < u^0, x> \\
		||\beta^0 (u^0)^T x|| &= |< u^0, x>| \cdot ||\beta^0||
	\end{align}

	Donc le $\sup$ est bien atteint en $x = u / ||u^0||$ et est égal à $||u^0|| \cdot ||\beta^0||$. En utilisant les approximations précédentes, on a donc $|\beta^0 (u^0)^T|_{op} \simeq m$. Ainsi on a $|\Delta A|_{op} \sim m^{2\alpha - 1} \cdot m^{-1/2 - \alpha} \cdot m \sim m^{\alpha - 1/2}.$ et :
	
	\[\frac{|\Delta A|_{op}}{|A^0|_{op}} \sim m^{\alpha - 1}\]
	
	\subsubsection{Choix de $\alpha$}
	
	\begin{itemize}
		\item[$\bullet$] $\alpha < 1$ \\
		
		Dans ce cas là, tous les écarts relatifs d'ordre $m^{\alpha - 1} \xrightarrow[m \to \infty]{} 0$. On a donc pour $m$ grand :
		
		\[||\Delta u|| \ll ||u^0||\]
		
		\[||\Delta \beta|| \ll ||\beta^0||\]
		
		\[|\Delta A|_{op} \ll |A^0|_{op}\]

		Regardons maintenant les $\Delta$ des gradients en $u$ pour la première itération :
		
		\begin{align}
			\Delta \nabla_u \Phi &\coloneqq \nabla_u \Phi (\theta^1, x) -  \nabla_u \Phi (\theta^0, x) \\
			&\sim m^{-\alpha - 1/2} \underbrace{((\beta^1)^T A^1 - (\beta^0)^T A^0)}_\text{($\star$)} \\
			 (\star) &= (\beta^0 - \eta \nabla_{\beta} F(\Phi(\theta^0, x)))^T (A^0 - \eta \nabla_{A} F(\Phi(\theta^0, x))) - (\beta^0)^T A^0 \\
			&= \eta^2 [\nabla_{\beta} F(\Phi)]^T[\nabla_{A} F(\Phi)] - \eta [\nabla_{\beta} F(\Phi)] A^0 - \eta \beta^0 [\nabla_{A} F(\Phi)] \\
			&= (\Delta \beta)^T(\Delta A) - (\Delta \beta) A^0 -  \beta^0 (\Delta A)
		\end{align}
	
	Donc :
	
	\begin{align}
		||\Delta \nabla_u \Phi|| &\sim m^{-\alpha - 1/2} ||(\Delta \beta)^T(\Delta A) - (\Delta \beta) A^0 -  \beta^0 (\Delta A)|| \\
		&\lesssim m^{-\alpha - 1/2} (||\Delta \beta|| \cdot |\Delta A|_{op} + ||\Delta \beta|| \cdot |A^0|_{op} + ||\beta^0|| \cdot |\Delta A|_{op}) \\
		&\lesssim m^{-\alpha - 1/2} (m^{\alpha - 1/2}m^{\alpha - 1/2} + m^{\alpha - 1/2}m^{1/2} + m^{1/2}m^{\alpha - 1/2}) \\
		&\lesssim m^{-\alpha - 1/2} (m^{\alpha - 1/2}m^{1/2} + m^{\alpha - 1/2}m^{1/2} + m^{1/2}m^{\alpha - 1/2}) \\
		&\lesssim m^{-1/2}
	\end{align}

	C'est-à-dire que $||\Delta \nabla_u \Phi|| = \mathcal{O}(m^{-1/2})$ pour $m$ grand. \\
	
	Par la même démarche on trouve $||\Delta \nabla_{\beta} \Phi|| = \mathcal{O}(m^{-1/2})$ et $||\Delta \nabla_{A} \Phi||_F = \mathcal{O}(m^{-1/2})$ pour $m$ grand. \\
	
	Ainsi :
	
	\[||\Delta \nabla_{\beta / u / A} \Phi|| = \mathcal{O}(m^{-1/2}) \stackrel{\alpha < 1}{\ll} m^{1/2 - \alpha} \sim ||\nabla_{\beta / u / A} \Phi||\]
	
	Cela traduit un comportement linéaire lorsque $\alpha < 1$ pour $m$ grand. \\
	
	On peut aussi remarquer que
	
	\[\Delta \nabla_{\theta} F(\Phi) =  F'(\Phi) \cdot \Delta \nabla_{\theta} \Phi \]
	
	On a aussi :

	\[||\Delta \nabla_{\beta / u / A} F(\Phi) || = \mathcal{O}(m^{-1/2}) \stackrel{\alpha < 1}{\ll} m^{1/2 - \alpha} \sim ||\nabla_{\beta / u / A} F(\Phi)||\]

	
		
	\end{itemize}


	
	
\end{document}