\documentclass[a4paper, 11pt, french]{article}
\usepackage[utf8]{inputenc}
\usepackage[margin=1.5cm,headheight=52pt,includeheadfoot]{geometry} 
\usepackage{mathtools} %amsmath mis à jour
\usepackage{amssymb} %symboles mathématiques
\usepackage{latexsym}
\usepackage{stmaryrd} 
\usepackage{babel} %gestion du français
\usepackage{amsthm}

\renewcommand{\labelitemi}{∙}

\newcommand{\abs}[1]{\vert#1\vert}
\newcommand{\norme}[1]{\Vert#1\Vert}
\newcommand{\scal}[2]{<#1\vert#2>}

\theoremstyle{definition}
\newtheorem{definition}{Définition}

\newtheorem{theorem}{Théorème}

\newtheorem{property}{Propriété}

\renewcommand\qedsymbol{$\blacksquare$}


\title{TER M1 : \\ NN and RKHS}
\author{Matthieu Denis}

\begin{document}
	
	\maketitle
	\newpage
	
	\tableofcontents
	\newpage
	
	\section{Reproducing Kernel Hilbert space (RKHS) et leurs applications}
	
	\subsection{Cadre théorique}
	
	Nous allons dans cette section s'intéresser aux RKHS, des espaces de Hilbert réels qui satisfont certaines propriétés, et qui ont des applications intéressantes en machine learning. Par exemple, on montrera un théorème qui nous permet de simplifier un problème de minimisation de risque empirique de dimension infini à un problème en dimension fini. Ou encore des applications dans plusieurs algorithmes de ML, les transformant d'algorithmes linéaires à non-linéaires à très faible prix, et d'autres encore. \\
	
	Soit $X$ un ensemble quelconque, $H$ un espace de Hilbert de fonctions réelles sur $X$, muni de l'addition point par point ainsi que de la multiplication par scalaire point par point. On introduit aussi une forme linéaire qui à chaque fonction de $H$ l'évalue en un certain point $x \in X$,
	
	\[L_x : f \mapsto f(x) \; \forall f \in H\]
		
	\begin{definition}[RKHS]
		On dit d'un espace de Hilbert qu'il est un RKHS si $\forall x \in X$, $L_x$ est continue sur $H$, ou encore si $L_x$ est bornée sur $H$, i.e
		\[\forall x \in X, \;  \exists M_x > 0, \; \forall f \in H \text{ t.q } |L_x(f)| \coloneqq |f(x)| \leq M_x ||f||_H\]
	\end{definition}

	Dans ce qui suit, $H$ sera un RKHS.

	\begin{property}[Convergence en norme dans un RKHS implique pointwise convergence]
		Soient $f_n, \, f \in H$. Si $f_n \stackrel{H}{\to} f$, alors $\forall x \in X, \; f_n(x) \to f(x)$.
	\end{property}
	\begin{proof}
		\begin{align*}
			\forall x \in X, \; |f_n(x) - f(x)| = |L_x(f_n) - L_x(f)| = |L_x(f_n - f)| \leq M_x||f_n - f||_H
		\end{align*}
	\end{proof}

	\begin{definition}[Noyau / Kernel]
		Une fonction $k : X \times X \to \mathbb{R}$ est un noyau si
		\[ \exists \phi : X \to H \text{ t.q } k(x,y) = \langle \phi (x), \phi(y) \rangle_H \; \forall x,y \in X \]
	\end{definition}

	\begin{property}
		Tout noyau $k$ est symétrique défini positif.
	\end{property}
	\begin{proof}
		\begin{itemize}
			\item[$\bullet$] Symétrie : découle de la symétrie du produit scalaire.			
			\item[$\bullet$] Défini positif : \\
			$\forall x_1, \cdots, x_n \in X, \forall c_1, \cdots, c_n \in X$
			\[\sum_{i,j=1}^{n} c_i c_j k(x_i, x_j) = \left\langle \sum_{i=1}^{n} c_i \phi(x_i), \sum_{j=1}^{n} c_j \phi(x_j) \right\rangle_H = \left|\left|\sum_{i=1}^{n} c_i \phi(x_i)\right|\right|_H^2\ \geq 0\]
		\end{itemize}
	\end{proof}

	\begin{definition}[Noyau reproduisant / Reproducing kernel]
		Une fonction $k : X \times X \to \mathbb{R}$ est un noyau reproduisant de $H$ si $\forall x \in X, \, f \in H$ :
		\begin{itemize}
			\item[$\bullet$] $k(\cdot, x) \in H$			
			\item[$\bullet$] $f(x) = \langle f, k(\cdot, x) \rangle_H$
		\end{itemize}
	\end{definition}

	\newpage

	\begin{property}
		Tout noyau reproduisant $k$ est un noyau.
	\end{property}
	\begin{proof}
		En prenant $f = k(\cdot, y) \in H$ pour un certain $y \in X$ dans la définition, on a en particulier $\forall x, y \in X \; k(x,y) = \langle k(\cdot, x), k(\cdot, y) \rangle_H$. Ici $\phi(u) = k(\cdot, u) \; \forall u \in X$.
	\end{proof}

	\begin{theorem}[Théorème de représentation de Riesz]
		Soient :
		\begin{itemize}
			\item[$\bullet$] $H$ un espace de Hilbert réel, muni de son produit scalaire $\langle \cdot, \cdot \rangle_H$
			\item[$\bullet$] $L \in H'$ une forme linéaire continue sur $H$.
		\end{itemize}
		Alors \[\exists ! \, g \in H, \; \forall f \in H, \; L(f) = \langle f, g \rangle_H\]
	\end{theorem}

	\begin{property}[Existence et Unicité]
		Il existe un unique noyau reproduisant de $H$.
	\end{property}
	\begin{proof}
		Montrons l'existence puis l'unicité : \\
		\begin{itemize}
			\item[$\bullet$]
			On applique le théorème de Riesz à $L_x$ :
			\[\forall x \in X, \; \exists ! \, k_x \in H, \; \forall f \in H, \; f(x) = L_x (f) = \langle f, k_x \rangle_H\]
			Soit $k(y, x) \coloneqq k_x(y) \; \forall x,y \in X$, alors $\forall x \in X, \, f \in H, \; k(\cdot, x) \in H$ et  $f(x) = \langle f, k(\cdot, x) \rangle_H$.
			
			Donc $k$ est un noyau reproduisant de $H$.
			
			\item[$\bullet$]
			Soient $k_1$ et $k_2$ deux noyau reproduisant de $H$. Alors $\forall x \in X, \, f \in H$
			\[\langle f, k_1(\cdot, x) - k_2(\cdot, x) \rangle_H = f(x) - f(x) = 0\]
			En prenant $f = k_1(\cdot, x) - k_2(\cdot, x)$, on a :
			\[\left|\left| k_1(\cdot, x) - k_2(\cdot, x) \right|\right|_H^2 = 0\]
			C'est-à-dire que $k_1 = k_2$ : le noyau reproduisant de $H$ est unique.
		\end{itemize}
	\end{proof}

	\begin{theorem}[Lien des deux visions]
		$H$ est un RKHS si et seulement si il existe un unique noyau reproduisant de $H$.
	\end{theorem}
	\begin{proof}
		\begin{itemize}
			\item[$\bullet$] $(\Rightarrow)$
				Donné par la propriété 2.
			\item[$\bullet$] $(\Leftarrow)$ $\forall x \in X, \, f \in H$
				\[|L_x(f)| = |f(x)| = |\langle f, k(\cdot, x) \rangle_H| \leq ||f||_H ||k(\cdot,x)||_H = \underbrace{\sqrt{k(x,x)}}_{\text{$M_x$}} ||f||_H\]
				$M_x$ ne dépendant pas de $f$, on a bien l'inégalité $\forall x \in X, \;  \exists M_x > 0, \; \forall f \in H$, i.e $H$ est un RKHS.
		\end{itemize}
	\end{proof}
	
	\begin{theorem}[Théorème de Moore-Aronszajn]
	 	Soit K un kernel. Alors il existe un unique espace de Hilbert $H$ de fonctions sur $X$ pour lequel $K$ est un noyau reproduisant.
	\end{theorem}
	\begin{proof}
		$\forall x \in X$, on pose $K_x(\cdot) \coloneqq K(x, \cdot)$. Soit $H_0$ le sous-espace vectoriel engendré par \\ $\{K_x : x \in X\}$. Sous couvert d'existence de $H$, la reproducing property de $K$ nous assure que $\forall x, y \in X, \; K(x, y) = \langle K_x, K_y \rangle_H$. On a de même :
		\[\left\langle \sum_{i=1}^{n} a_i K_{x_i}, \sum_{j=1}^{m} b_j K_{x_j} \right\rangle_H = \sum_{i=1}^{n} \sum_{j=1}^{m} a_i b_j K(x_i, x_j)\]
		
		Il est alors naturel de définir pour toutes fonctions de $H_0$ : $f \coloneqq \sum_{i=1}^n a_i K_{x_i}$ et $g \coloneqq \sum_{j=1}^m b_i K_{y_j}$ 
		\[\langle f, g \rangle_{H_0} \coloneqq \sum_{i=1}^{n} \sum_{j=1}^{m} a_i b_j K(x_i, x_j) = \sum_{i=1}^n a_i g(x_i) = \sum_{j=1}^m b_j f(y_j)\]
		
		Déjà, $K_x \in H_0$, et comme $f(x) = \langle f, K_x \rangle_{H_0} < \infty$, $\langle \cdot, \cdot \rangle_{H_0}$ est bien défini, billinéaire, est symétrique et positif, et on a la reproducing property :
		
		\[\forall x \in X, \; f \in H_0, \; \langle f, K_x \rangle_{H_0} = \sum_{i=1}^n a_i K_x(x_i) = \sum_{i=1}^n a_i K_{x_i}(x) = f(x)\]
		
		Il reste à montrer que $||f||_{H_0} = 0 \Rightarrow f = 0$ pour que ca soit un produit scalaire :
		
		\[|f(x)| = |\langle f, K_x \rangle_{H_0}| \leq ||f||_{H_0} ||K_x||_{H_0} = ||f||_{H_0} \sqrt{K(x,x)} \]
		
		Donc $\langle \cdot, \cdot \rangle_{H_0}$ est un produit scalaire. \\
		
		On complète $H_0$ avec le produit scalaire $\langle \cdot, \cdot \rangle_{H_0}$ en $H$, et toutes les propriétés sont gardées. \\
		
		Unicité : supposons $G$ un autre espace de Hilbert pour lequel $K$ est un noyau reproduisant. 
		\begin{itemize}
			\item[$\bullet$]
			$(\subseteq)$ : \\
			On a
			\[\forall x, y \in X, \; \langle K_x, K_y \rangle_H = K(x, y) = \langle K_x, K_y \rangle_G\]
			Par linéarité, $\langle \cdot, \cdot \rangle_H = \langle \cdot, \cdot \rangle_G$ sur tout $H_0$, i.e $H_0 \subseteq G$. Donc $H \subseteq G$ car $G$ est complet.
			
			\item[$\bullet$]
			$(\supseteq)$ : \\
			Soit $f \in G$. Comme $H$ est un sous-espace fermé de $G$, on peut écrire $f = f_{H} + f_{H^\perp}$ par le théorème de décomposition orthogonal. De plus comme $K$ est un noyau reproductif de $G$ et $H$ on a $\forall x \in X$ :
			
			\[f(x) = \langle K_x, f \rangle_G = \langle K_x, f_{H} \rangle_G + \langle K_x, f_{H^\perp} \rangle_G = \langle K_x, f_{H} \rangle_G = \langle K_x, f_{H} \rangle_H = f_H (x) \]
			
			Ainsi $f \in H$.
		\end{itemize}	
	\end{proof}
	
	Tout compte fait, on a montré que :
	\[H \text{ est un RKHS} \Leftrightarrow \exists ! \; k \text{ noyau reproduisant de $H$} \Leftrightarrow k \text{ est un noyau}\]
	
	Regardons quelques kernels classiques :
	\begin{itemize}
		\item[$\bullet$] $k(x, y) \coloneqq \langle x, y \rangle$, son RKHS est $H = \{\langle \cdot, \beta \rangle : ||\langle \cdot, \beta \rangle||_H^2 = ||\beta||^2\}$
		\item[$\bullet$] $k(x, y) \coloneqq (\alpha \langle x, y \rangle + 1)^d, \; \alpha \in \mathbb{R}, \; d \in \mathbb{N}$
		\item[$\bullet$] $k(x, y) \coloneqq \exp(||x-y||^2 / (2\sigma^2)), \; \sigma > 0$
		\item[$\bullet$] $k(x, y) \coloneqq \exp(||x-y|| / \sigma), \; \sigma > 0$
		\item[$\bullet$] $k(x, y) \coloneqq \sin (a(x-y)) / \pi (x-y)$, son RKHS correspond aux fonctions de $L^2(\mathbb{R})$ dont la transformée de fourier est à support dans $[-a, a]$.
	\end{itemize}
	
	\subsection{Applications des RKHS en machine learning}
	
	Une application très importante des RKHS est le Representer theorem, un théorème qui permet de transformer des problèmes d'optimisation de dimension infinie en dimension finie, qu'on pourra alors résoudre avec les méthodes d'optimisation numérique. On verra son application avec le Kernel Ridge Regression et avec les SVM.
	
	\subsubsection{Representer theorem}
	
	\begin{theorem}[Representer theorem]
		Soit $k$ un kernel sur $X$ et soit $H$ sa RKHS associée. Posons $x_1, \cdots, x_n \in X$ notre training sample. Regardons le problème d'optimisation suivant :
		
		\[\min_{f \in H} \; J(f) \coloneqq E(f(x_1), \cdots, f(x_n)) + P(||f||_H^2)\]
		Où $P$ est une fonction croissante. \\
		
		Alors si ce problème d'optimisation a (au-moins) une solution, il y a (au-moins) une solution de la forme \[f = \sum_{i=1}^{n} \alpha_i \cdot k(\cdot, x_i)\].
		
		De plus, si $P$ est strictement croissante, alors toute solution a cette forme.
	\end{theorem}
	\begin{proof}
		Soit $H_0$ le sous espace engendré par $\{k(\cdot, x_i ) : i \in 1, \cdots, n\}$. Comme $H_0 \in H$, $H_0$ est fermé car de dimension finie. Alors le théorème de décomposition orthogonale nous dit que : $\forall f \in H, \; f = f_{H_0} + f_{{H_0}^\perp}$. \\
		
		De plus, comme $k$ est un noyau reproductif de $H$, on a $\forall x_i$ :
		
		\[f(x_i) = \langle k(\cdot, x_i), f \rangle_H = \langle k(\cdot, x_i), f_{H_0} \rangle_H + \langle k(\cdot, x_i), f_{{H_0}^\perp} \rangle_H = \langle k(\cdot, x_i), f_{H_0} \rangle_H = f_{H_0} (x_i) \]
		
		Alors :
		
		\begin{align*}
			J(f) &\coloneqq E(f(x_1), \cdots, f(x_n)) + P(||f||_H^2) \\
			&= E(f_{H_0}(x_1), \cdots, f_{H_0}(x_n)) + P(||f||_H^2) \\
			&\geq E(f_{H_0}(x_1), \cdots, f_{H_0}(x_n)) + P(||f_{H_0}||_H^2) \text{ par croissance de $P$ car $||f||_H^2 = ||f_{H_0}||_H^2 + ||f_{{H_0}^\perp}||_H^2$}\\
			&= J(f_{H_0})
		\end{align*}
	
		Donc si $f$ est un minimiseur de $J$, alors $f_{H_0} \coloneqq \sum_{i=1}^{n} \alpha_i \cdot k(\cdot, x_i)$ aussi. Si $P$ est strictement croissante, alors l'inégalité est stricte et si l'on veut un minimiseur de $J$ il faut nécessairement qu'il soit de cette forme.
	\end{proof}

	\subsubsection{Exemple 1 : Kernel Ridge Regression}
	
	Ici, $J(f) \coloneqq \sum_{i=1}^{n} (y_i - f(x_i))^2 + \lambda ||f||_H^2$, prenons $k$ un kernel sur $X$. Le representer theorem nous dit que la solution de ce problème (sous couvert d'existence) est nécessairement de la forme 
	\[f = \sum_{i=1}^{n} \alpha_i \cdot k(\cdot, x_i)\]
	Rappelons que :
	
	\[||f||_H^2 = \left\langle \sum_{i=1}^{n} a_i k(\cdot, x_i), \sum_{j=1}^{n} a_j k(\cdot, x_j) \right\rangle_H = \sum_{i=1}^{n} \sum_{j=1}^{n} a_i a_j K(x_i, x_j)\]
	
	Le problème d'optimisation \[\min_{f \in H} \; J(f) \coloneqq \sum_{i=1}^{n} (y_i - f(x_i))^2 + \lambda ||f||_H^2\]
	
	est alors équivalent à 
	
	\[\min_{\alpha \in \mathbb{R}^n} \; \sum_{i=1}^{n} (y_i - \sum_{j=1}^{n} \alpha_j \cdot k(x_i, x_j))^2 + \lambda \sum_{i=1}^{n} \sum_{j=1}^{n} a_i a_j K(x_i, x_j)\]
	
	ou encore avec $(K)_{ij} = k(x_i, x_j)$ (qui est symétrique)
	
	\[\min_{\alpha \in \mathbb{R}^n} \; F(\alpha) \coloneqq ||y - K \alpha||_2^2 + \lambda \alpha^T K \alpha\]
	
	Reste à résoudre ce problème de la même manière qu'une régression ridge simple :
	
	\[\nabla_{\alpha} F(\alpha) = -2 K (y - K \alpha) + 2\lambda K \alpha\]
	
	\[\nabla_{\alpha} F(\alpha) = 0 \Leftrightarrow \alpha = (K + \lambda Id_n)^{-1} y\]
	
	Notre prédicteur sera ainsi
	
	\[f = \sum_{i=1}^{n} ((K + \lambda Id_n)^{-1} y)_i \cdot k(\cdot, x_i)\]
	
	Ce qui est mieux par rapport à la régression pénalisée, c'est que notre prédicteur est une fonction non-linéaire de $x$, nous permettant d'aller chercher dans la classe des fonctions de la RKHS associée à $k$ (et donc potentiellement avoir de meilleures prédictions).
	
	\subsubsection{Exemple 2 : Support Vector Machine (SVM)}	
	
	Dans le cadre d'une SVM, $J(f) \coloneqq \frac{1}{n} \sum_{i=1}^{n} max(0, 1 - y_i f(x_i)) + \frac{\lambda}{2} ||f||_H^2$. Prenons encore une fois un kernel $k$ sur $X$. Encore une fois, le representer theorem nous dit que la seule solution (si elle existe) est sous la forme
	\[f = \sum_{i=1}^{n} \alpha_i \cdot k(\cdot, x_i)\]
	
	On cherche alors à résoudre le problème suivant :
	\[\min_{\alpha \in \mathbb{R}^n} \; \sum_{i=1}^{n} max(0, 1 - y_i \sum_{j=1}^{n} \alpha_j \cdot k(x_i, x_j))  + \frac{\lambda}{2} \sum_{i=1}^{n} \sum_{j=1}^{n} a_i a_j K(x_i, x_j)\]
	
	 On peut montrer que le dual de ce problème est :
	\[\min_{\gamma \in \mathbb{R}^n} \; -\sum_{i=1}^{n} \gamma_i + \frac{1}{2} \sum_{i=1}^{n} \sum_{j=1}^{n} \gamma_i \gamma_j y_i y_j k(x_i, x_j) \text{ t.q } 0 \leq \gamma_i \leq \frac{1}{n \lambda} \; \forall i \in \{1, \cdots, n\}\]
	
	avec $\alpha_i = y_i \gamma_i \; \forall i \in \{1, \cdots, n\}$. On trouve la la solution du dual par des algorithmes d'optimisation quadratique. \\
	
	Le prédicteur est donc :
	
	\[f = \sum_{i=1}^{n} y_i \gamma_i \cdot k(\cdot, x_i)\]
	
	On a remplacé l'estimateur linéaire de la SVM par un estimateur non-linéaire de $x$, sans changer la complexité de l'algorithme.
	
	\subsubsection{Le Kernel Trick}
	
	Plus généralement, à tout algorithme qui utilise des produits scalaires, on peut les remplacer par un kernel. Ainsi on peut transformer rendre non-linéaires les algorithmes, en manipulant des vecteurs de dimensions infinie sans que cela pose problème, comme leur produit scalaire est égal au kernel. C'est ce qu'on appelle le "kernel trick", et il a une très grande importance dans les applications pratiques. Par exemple, ce que l'on a fait plus haut avec peut aussi être fait avec le kernel trick dans la preuve de la construction de la solution linéaire en remplacant $\langle \cdot, \cdot \rangle$ par $\langle \phi(\cdot), \phi(\cdot) \rangle = k(\cdot, \cdot)$.
	
	
	
	\section{Introduction : Réseau de neurone simple}
	
	Commencons par étudier un NN très simple : une fonction
	$\Phi : (\mathbb{R}^m \times \mathbb{R}^{m \times m} \times \mathbb{R}^m) \times \mathbb{R} \to \mathbb{R}$ combinaison d'applications linéaires, sans non linéarités  intermédiaires  :
	
	\[ \Phi ((\beta, A, u), x) \coloneqq \frac{1}{m^{\alpha}} \beta^T
		 \left( \frac{1}{m^{\gamma}} A \right) u x \]
		 
	On initialise $\theta^0 \coloneqq (\beta^0, A^0, u^0)$ de manière standarde : 
	$ \forall i,j \in \{1, \cdots, m\} , \; u_i^0, A_{ij}^0, \beta_i^0 \sim_{iid} N(0, 1)$
	
	Nous montrerons quelques propriétés asymptotiques en la largeur des couches, et sur l'évolution des paramètres lors du premier pas de la descente de gradient.
	
	\subsection{Lois suivant la largeur des couches $m$}
	
	\begin{itemize}
		
		\item[$\bullet$] Loi de $ || u^0 ||_2^2$ pour $m$ grand \\
		
		Comme $ || u^0 ||_2^2 = \sum_{i=1}^{m} (u_i^0)^2 \sim \chi^2 (m)$ et $ u_i^0 \sim \chi^2 (1)$, en appliquant le TCL aux $ u_i^0 $, on a :
		
		\[
			\frac{|| u^0 ||_2^2 - m}{\sqrt{2m}} \sim_{m \to \infty}  N(0, 1)
		\]
	
		C'est-à-dire que $ || u^0 ||_2^2 \sim N(m, 2m) $  pour $m$ grand. \\
		
		\item[$\bullet$] Loi de $ \left(\frac{1}{m^{\gamma}} A^0 \right) u^0 x $ sachant $ u^0 $ \\
		
		$ (A^0 u^0)_i = \sum_{j=1}^m A_{ij}^0 u_j^0$. En sachant $u^0$, comme 
		$A_{i \cdot}^0$ est un vecteur gaussien, $(A^0 u^0)_i \sim  N(0, || u^0 ||_2^2 ) $. 
	
		De même, part indépendance des $A_{ij}^0$, les $(A^0 u^0)_i$ sont indépendants et $A^0 u^0 \sim N(0_m, || u^0 ||_2^2 \; Id_m)$.
		
		Ainsi, $ \left(\frac{1}{m^{\gamma}} A^0 \right) u^0 x $ | $ u^0 
		\sim  N(0_m, \left( \frac{x}{m^{\gamma}} \right)^2 || u^0 ||_2^2 \; Id_m) $. \\
		
		\item[$\bullet$] Loi de $ \left(\frac{1}{m^{\gamma}} A^0 \right) u^0 x $ \\
		
		HISTOIRE DE MEME TRIBU ENGENDREE PAR U0 ET LE RESTE IMPLIQUE QUE CEST LA MEME CHOSE DECONDITIONNEE
		
		Donc $ \left(\frac{1}{m^{\gamma}} A^0 \right) u^0 x  \sim  N(0_m, 
		\left( \frac{x}{m^{\gamma}} \right)^2 || u^0 ||_2^2 \; Id_m) $. \\
		
		\item[$\bullet$] Loi de $ || \left(\frac{1}{m^{\gamma}} A^0 \right) u^0 x \; ||_2^2 $ pour $m$ grand
		 \\
		
		On a $ \left( \left(\frac{1}{m^{\gamma}} A^0 \right) u^0 x \right)_i^2 \sim  
		\left( \frac{x}{m^{\gamma}} \right)^2 || u^0 ||_2^2 \cdot  \chi^2 (1) $, d'espérance 
		$ \mu \coloneqq \left( \frac{x}{m^{\gamma}} \right)^2 || u^0 ||_2^2  $  et de variance \\
		$ \sigma^2 \coloneqq 2 \left( \left( \frac{x}{m^{\gamma}} \right)^2 || u^0 ||_2^2  \right)^2 $.
		
		Donc en appliquant le TCL à ceux ci, on a :
		
		 \[
		 	\frac{|| \left(\frac{1}{m^{\gamma}} A^0 \right) u^0 x \; ||_2^2 - m \mu}{\sigma \sqrt{m}} \sim_{m \to \infty}  N(0, 1)
		 \]
		
		C'est-à-dire que $|| \left(\frac{1}{m^{\gamma}} A^0 \right) u^0 x \; ||_2^2 \sim 
		N(m \mu, m \sigma^2) $  pour $m$ grand. \\
		
		\newpage
		
		\item[$\bullet$][$\bullet$] Loi de $ \frac{1}{m^{\alpha}} (\beta^0)^T x_2 $ sachant $x_2$, avec 
		$x_2 = \left(\frac{1}{m^{\gamma}} A^0 \right) u^0 x$ \\
		
		On a 
		$ \frac{1}{m^{\alpha}} (\beta^0)^T x_2 | x_2 \sim N(0,  \frac{1}{m^{2\alpha}}||x_2||_2^2) $
		\\
		
		\item[$\bullet$][$\bullet$] Loi de $ \frac{1}{m^{\alpha}} (\beta^0)^T x_2 $ \\
		
		On a 
		$ \frac{1}{m^{\alpha}} (\beta^0)^T x_2 \sim N(0,  \frac{1}{m^{2\alpha}}||x_2||_2^2) $
		\\
		
	\end{itemize}

	\subsection{Choix de $\gamma$}
	
	On regarde la variance de chaque composante de 
	$ \left(\frac{1}{m^{\gamma}} A^0 \right) u^0 x $ pour $m$ grand : 
	$ Var = x^2 m^{-2\gamma} m $ comme $|| u^0 ||_2^2$ se comporte en $m$ pour $m$ grand. Or on ne veut pas qu'elle tende vers 0 ou l' infini lorsque $m$ tend vers l'infini car $\Phi$ prendrait des valeurs de 0 ou l'infini, ce qui impose le choix $\gamma = 1/2$ \\
	
	Dans la suite, on prendra $\gamma = 1/2$.

	\subsection{Gradients}
	
	Trivialement,
	
	\[\nabla_{\beta} \Phi = \frac{x}{m^{\alpha + 1/2}} A u\]
	
	\[\nabla_u \Phi = \frac{x}{m^{\alpha + 1/2}} A^T \beta\]

	\[\nabla_A \Phi = \frac{x}{m^{\alpha + 1/2}} \beta u^T\]
	
	Etudions les ordres de grandeur des normes correspondantes à l'initialisation pour $m$ grand :
	
	On va simplement utiliser les approximations données par la loi des grands nombres :
	 $||u^0|| \simeq \sqrt{m}$, et comme vu plus haut, 
	 $||A^0 u^0|| \simeq \sqrt{m}||u^0|| \simeq m$. Ainsi $||\nabla_{\beta} \Phi (\theta^0, x)|| \sim m^{-\alpha - 1/2} \cdot m = m^{1/2 - \alpha}$.
	 
	 \[||\nabla_{\beta} \Phi (\theta^0, x)|| \sim m^{1/2 - \alpha}\]
	 
	 Exactement de la même manière, on aboutit à :
	 
	 \[||\nabla_u \Phi (\theta^0, x)|| \sim m^{1/2 - \alpha}\]
	 
	 Pour $A$, on prend la norme de Frobenius : la LGN nous dit que $||\beta^0 (u^0)^T|| \simeq m$ et donc :
	 
	 \[||\nabla_A \Phi (\theta^0, x)||_F \sim m^{1/2 - \alpha}\]
	 
	 \subsection{Descente de gradient}
	 
	 On va étudier ici le premier pas de descente de gradient.
	 
	 Posons une fonction de perte $F : \mathbb{R} \rightarrow \mathbb{R}$ t.q $F'(0) \neq 0$ et 
	 $\Delta F \coloneqq F(\Phi(\theta^1, x)) - F(\Phi(\theta^0, x))$, avec 
	 $\theta^1 \coloneqq \theta^0 - \eta \nabla_{\theta} F(\Phi(\theta^0, x))$
	 
	 Il semble honnête de prendre $\eta$ dépendant de $m$, le produit scalaire final ayant plus de chance d'exploser en grande dimension. Prenons $\eta \coloneqq m^a$, $a \in \mathbb{R}$ \\
	 
	 \subsubsection{Choix de $\eta$}
	 
	 On veut que $\Delta F$ ne diverge pas ni ne tende vers 0 lorsque m tend vers l'infini.
	 
	 Pour cela, on utilise l'approximation 
	 $\Delta F \simeq \; < \Delta \theta, \nabla_{\theta} F(\Phi(\theta^0, x)) >$.
	 
	 On a 
	 \[
	 \Delta F \simeq \; < -\eta \nabla_{\theta} F(\Phi(\theta^0, x)) , \nabla_{\theta} F(\Phi(\theta^0, x)) > = -\eta || \nabla_{\theta} F(\Phi(\theta^0, x)) ||^2
	 \]
	 
	 \[
	 \nabla_{\theta} F(\Phi(\theta^0, x)) = 
	 \underbrace{F'(\Phi(\theta^0, x))}_\text{constante en $m$} 
	 \cdot \nabla_{\theta} \Phi(\theta^0, x)
	 \]
	 
	 Or $ || \nabla_{\theta} \Phi(\theta^0, x) ||^2 = || \nabla_{\beta} \Phi(\theta^0, x) ||^2 + || \nabla_{u} \Phi(\theta^0, x) ||^2 + || \nabla_{A} \Phi(\theta^0, x) ||^2$ \\
	 
	 Donc pour $m$ grand :
	 \begin{align*}
	 	\Delta F &\simeq -\eta || \nabla_{\theta} F(\Phi(\theta^0, x)) ||^2 \\
	 	&\sim \eta || \nabla_{\theta} \Phi(\theta^0, x) ||^2 \\
	 	&\simeq \eta (3 \cdot (m^{1/2 - \alpha})^2) \\
	 	&\simeq m^a \cdot m^{1 - 2\alpha}
	 \end{align*}	
	
	Ce qui impose le choix $a = 2\alpha - 1$ \\
	
	\subsubsection{Ordres de grandeur des écarts relatifs}

	Pour cela introduisons $\Delta \theta \coloneqq \theta^1 - \theta^0$. Remarquons que $	\Delta \theta = - \eta \nabla_{\theta} F(\Phi(\theta^0, x))$.
	
	\begin{align}
		||\Delta u|| &\sim \eta || \nabla_u \Phi(\theta^0, x) || \\
		&\sim m^{2\alpha - 1} \cdot m^{1/2 - \alpha} \\
		&\sim m^{\alpha - 1/2}
	\end{align}

	Ce qui nous donne un ordre de grandeur de l'écart relatif :
	
	\[\frac{||\Delta u||}{||u^0||} \sim m^{\alpha - 1}\]
	
	On a le même résultat pour l'écart relatif de $\beta^0$ :
	
	\[\frac{||\Delta \beta||}{||\beta^0||} \sim m^{\alpha - 1}\]
	
	Pour $A$, la LGN nous donne $||A^0||_F \simeq m$ pour $m$ grand, on a alors par les mêmes calculs :
	
	\[\frac{||\Delta A||_F}{||A^0||_F} \sim m^{\alpha - 3/2}\]
	
	Concernant l'écart entrywise de A, on a $|\Delta A_{ij}| \sim \eta | (\nabla_A \Phi(\theta^0, x))_{ij} | \sim m^{2\alpha - 1} \cdot m^{-1/2 - \alpha} \cdot 1 \sim m^{\alpha - 3/2}$ car $|\beta^0 (u^0)^T| \sim 1$. $|A_{ij}| \sim 1$, donc :

	\[\frac{|\Delta A_{ij}|}{|A_{ij}|} \sim m^{\alpha - 3/2}\]
	
	Maintenant avec la norme opétateur : le corollaire 7.9 du cours de MIA2 nous donne une majoration sur $|A^0|_{op}$ : $|A^0|_{op} \leq \sqrt{m} + 7\sqrt{m + \xi} \sim \sqrt{m}$, avec $\xi \sim Exp(1)$.
	
	De plus, on peut trouver la la norme opérateur de $\Delta A$ comme suit : tout le travail est de trouver $|\beta^0 (u^0)^T|_{op} \coloneqq \sup \{||\beta^0 (u^0)^T x|| \; 
	\text{avec }||x|| = 1\}$.
	
	\begin{align}
		(\beta^0 (u^0)^T x)_{ij} &= \beta^0_i u^0_j \\
		(\beta^0 (u^0)^T x)_i &= \sum_{j=1}^{m} (\beta^0 (u^0)^T x)_{ij} \cdot x_j \\
		&= \beta^0_i < u^0, x> \\
		||\beta^0 (u^0)^T x|| &= |< u^0, x>| \cdot ||\beta^0||
	\end{align}

	Donc le $\sup$ est bien atteint en $x = u / ||u^0||$ et est égal à $||u^0|| \cdot ||\beta^0||$. En utilisant les approximations précédentes, on a donc $|\beta^0 (u^0)^T|_{op} \simeq m$. Ainsi on a $|\Delta A|_{op} \sim m^{2\alpha - 1} \cdot m^{-1/2 - \alpha} \cdot m \sim m^{\alpha - 1/2}.$ et :
	
	\[\frac{|\Delta A|_{op}}{|A^0|_{op}} \sim m^{\alpha - 1}\]
	
	\subsubsection{Choix de $\alpha$}
	
	\begin{itemize}
		\item[$\bullet$] $\alpha < 1$ \\
		
		Dans ce cas là, tous les écarts relatifs d'ordre $m^{\alpha - 1} \xrightarrow[m \to \infty]{} 0$. On a donc pour $m$ grand :
		
		\[||\Delta u|| \ll ||u^0||\]
		
		\[||\Delta \beta|| \ll ||\beta^0||\]
		
		\[|\Delta A|_{op} \ll |A^0|_{op}\]

		Regardons maintenant les $\Delta$ des gradients en $u$ pour la première itération :
		
		\begin{align}
			\Delta \nabla_u \Phi &\coloneqq \nabla_u \Phi (\theta^1, x) -  \nabla_u \Phi (\theta^0, x) \\
			&\sim m^{-\alpha - 1/2} \underbrace{((\beta^1)^T A^1 - (\beta^0)^T A^0)}_\text{($\star$)} \\
			 (\star) &= (\beta^0 - \eta \nabla_{\beta} F(\Phi(\theta^0, x)))^T (A^0 - \eta \nabla_{A} F(\Phi(\theta^0, x))) - (\beta^0)^T A^0 \\
			&= \eta^2 [\nabla_{\beta} F(\Phi)]^T[\nabla_{A} F(\Phi)] - \eta [\nabla_{\beta} F(\Phi)] A^0 - \eta \beta^0 [\nabla_{A} F(\Phi)] \\
			&= (\Delta \beta)^T(\Delta A) - (\Delta \beta) A^0 -  \beta^0 (\Delta A)
		\end{align}
	
	Donc :
	
	\begin{align}
		||\Delta \nabla_u \Phi|| &\sim m^{-\alpha - 1/2} ||(\Delta \beta)^T(\Delta A) - (\Delta \beta) A^0 -  \beta^0 (\Delta A)|| \\
		&\lesssim m^{-\alpha - 1/2} (||\Delta \beta|| \cdot |\Delta A|_{op} + ||\Delta \beta|| \cdot |A^0|_{op} + ||\beta^0|| \cdot |\Delta A|_{op}) \\
		&\lesssim m^{-\alpha - 1/2} (m^{\alpha - 1/2}m^{\alpha - 1/2} + m^{\alpha - 1/2}m^{1/2} + m^{1/2}m^{\alpha - 1/2}) \\
		&\lesssim m^{-\alpha - 1/2} (m^{\alpha - 1/2}m^{1/2} + m^{\alpha - 1/2}m^{1/2} + m^{1/2}m^{\alpha - 1/2}) \\
		&\lesssim m^{-1/2}
	\end{align}

	C'est-à-dire que $||\Delta \nabla_u \Phi|| = \mathcal{O}(m^{-1/2})$ pour $m$ grand. \\
	
	Par la même démarche on trouve $||\Delta \nabla_{\beta} \Phi|| = \mathcal{O}(m^{-1/2})$ et $||\Delta \nabla_{A} \Phi||_F = \mathcal{O}(m^{-1/2})$ pour $m$ grand. \\
	
	Ainsi :
	
	\[||\Delta \nabla_{\beta / u / A} \Phi|| = \mathcal{O}(m^{-1/2}) \stackrel{\alpha < 1}{\ll} m^{1/2 - \alpha} \sim ||\nabla_{\beta / u / A} \Phi||\]
	
	Cela traduit un comportement linéaire lorsque $\alpha < 1$ pour $m$ grand. \\
	
	On peut aussi remarquer que
	
	\[\Delta \nabla_{\theta} F(\Phi) =  F'(\Phi) \cdot \Delta \nabla_{\theta} \Phi \]
	
	On a aussi :

	\[||\Delta \nabla_{\beta / u / A} F(\Phi) || = \mathcal{O}(m^{-1/2}) \stackrel{\alpha < 1}{\ll} m^{1/2 - \alpha} \sim ||\nabla_{\beta / u / A} F(\Phi)||\]

	
		
	\end{itemize}


	
	
\end{document}